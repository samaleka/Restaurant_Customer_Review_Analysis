{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16037,"status":"ok","timestamp":1709580941558,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"Gxv2UhpdHMFS","outputId":"f8c18e3f-58a0-4cbd-f94c-d65c9f6fc64d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-crfsuite\n","  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-crfsuite\n","Successfully installed python-crfsuite-0.9.10\n"]}],"source":["!pip install python-crfsuite"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucK5_I1xHSBi"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# NLP imports\n","import string\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","\n","import pycrfsuite\n","from sklearn.metrics import classification_report\n","\n","# misc imports\n","from ast import literal_eval\n","import glob"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":174140,"status":"ok","timestamp":1709581120927,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"mKsqua8-2KCb","outputId":"43f078b0-adae-4164-b853-4338a384652e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6P-jgAhsB31"},"outputs":[],"source":["# Provide Base path to Yelp Dataset\n","dataset_root = '/content/drive/MyDrive/UMich Milestone II Project/Final_Code_Submission/Dataset/NER/Yelp/'"]},{"cell_type":"markdown","metadata":{"id":"Zec4QU-NZwB5"},"source":["# CRF (Conditional Random Fields)\n","References:\n","\n","\n","1.   https://python-crfsuite.readthedocs.io/en/latest/\n","2.   https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541\n","3. https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463\n","4. https://dev.to/fferegrino/conditional-random-fields-in-python-sequence-labelling-part-4-5ei2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":959,"status":"ok","timestamp":1709581121883,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"CwrQpFrLZcpQ","outputId":"8c4d9342-c647-4fb5-bbf9-896e5c03ad3b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"nQ8BLDngxRS_"},"source":["### Data Pre processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cOnJty7xZ_2"},"outputs":[],"source":["# Get all NER datasets in root directory\n","file_list = glob.glob(dataset_root + 'yelp_NER_sample_*.csv')\n","\n","dfs = []\n","for file in file_list:\n","    df = pd.read_csv(file)\n","    dfs.append(df)\n","\n","# Concatenate all DataFrames into a single DataFrame\n","crf_df = pd.concat(dfs, ignore_index=True)\n","\n","# Drop reviews with missing annotations\n","crf_df = crf_df.dropna(subset=['ner_results'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edGDUhc_bI79"},"outputs":[],"source":["# Function to convert text into (word, pos_tag, ner_label) tuples\n","def convert_text_to_tuples(text, ner_results):\n","\n","    # print(f\"review: {text}\")\n","    # print(f\"NER: {ner_results}\")\n","\n","    # lowercasing text\n","    text = text.lower()\n","    # Tokenize the text\n","    words = word_tokenize(text, language='english')\n","    # removing punctuation\n","    words = [word for word in words if word not in string.punctuation]\n","\n","    # stop word removal\n","    stop_words = stopwords.words('english')\n","    words = [word for word in words if word not in stop_words]\n","\n","\n","    # Get POS tags for the words\n","    pos_tags = pos_tag(words)\n","\n","    # Initialize list to store (word, pos_tag, ner_label) tuples\n","    word_tuples = []\n","\n","    # Convert ner_results from string to dictionary\n","    ner_results_dict = literal_eval(ner_results)\n","    if type(ner_results_dict) != dict:\n","        print(f\"caught unexpected NER type: {ner_results}\")\n","        return word_tuples\n","\n","    # Check only ('food', 'drink', 'None') exist in NER lables\n","    valid_NER_Lables = ['food', 'drink', 'None']\n","    for lable in ner_results_dict.keys():\n","        if lable not in valid_NER_Lables:\n","            # print(ner_results_dict)\n","            return word_tuples\n","\n","    # Iterate through words and POS tags\n","    i = 0\n","    while i < len(words):\n","        # Check if the word is a named entity (food/drink item)\n","        ner_label = None\n","        for label, entities in ner_results_dict.items():\n","            # catch unexpected NER entities\n","            try:\n","                # lower case valid food/drink terms for word match\n","                entities_lower = [item.lower() for item in entities]\n","            except:\n","                print(f\"caught unexpected NER: {ner_results}\")\n","                return word_tuples\n","\n","            # loop through each valid food/drink item\n","            for food_drink_item in entities:\n","                # check if given word is part of food/drink item\n","                possible_food_drink = ' '.join(words[i:i+len(food_drink_item.split())]).lower()\n","                # print(f\"possible food/drink: {possible_food_drink}\")\n","                if possible_food_drink in entities_lower:\n","                    # print(f\"found a match {possible_food_drink}\")\n","                    match_food_drink = possible_food_drink.split()\n","                    ner_label = label\n","                    # Add (word, pos_tag, ner_label) tuple for each word in the named entity\n","                    for j in range(len(match_food_drink)):\n","                        word_tuples.append((words[i+j], pos_tags[i+j][1], ner_label))\n","                    i += len(match_food_drink) - 1  # Skip the next words as they are part of the named entity\n","                    break\n","        if ner_label is None:\n","            # If not a named entity, add (word, pos_tag, None) tuple\n","            word_tuples.append((words[i], pos_tags[i][1], 'None'))\n","        i += 1\n","    return word_tuples\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125764,"status":"ok","timestamp":1709581254730,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"rfXyV3lYdVt7","outputId":"180bffab-06a9-4607-e084-20e9a7bcce17"},"outputs":[{"output_type":"stream","name":"stdout","text":["caught unexpected NER type: [{'type': 'food', 'name': 'Mac n cheese'}, {'type': 'drink', 'name': 'beer'}]\n","caught unexpected NER: {'food': [{'item': 'Tuna Tartare', 'rating': 4.5}, {'item': 'Spinach and Asparagus', 'rating': 4.5}, {'item': 'Chilean Sea Bass', 'rating': 4.5}, {'item': 'Crab Cake', 'rating': 4.5}], 'drink': []}\n","caught unexpected NER: {'food': [{'name': 'Lynchburg Basil Lemonade', 'description': 'Jack Daniels, Fresh Basil, Homemade Southern Lemonade'}, {'name': 'Black Cherry Gimlet', 'description': 'Black Cherry Vodka, Vanilla, Lime, maraschino Cherry'}], 'drink': [{'name': 'Lynchburg Basil Lemonade', 'description': 'Jack Daniels, Fresh Basil, Homemade Southern Lemonade'}, {'name': 'Black Cherry Gimlet', 'description': 'Black Cherry Vodka, Vanilla, Lime, maraschino Cherry'}]}\n","caught unexpected NER: {'food': [{'name': 'Foie Gras Soup', 'description': 'Perfect combination of richness and spice, and just awesome.', 'rating': 5}, {'name': 'Pork Belly in Chili Oil', 'description': 'Very thinly sliced, but great combination of flavors and not too spicy at all.', 'rating': 5}, {'name': 'Gem lettuce', 'description': 'Basically a deconstructed Caesar salad. The red peppery looking things were really the highlight.', 'rating': 5}, {'name': 'Steak tartar', 'description': \"Also awesome, can't remember specific details.\", 'rating': 5}, {'name': 'Octopus in Piri Piri sauce', 'description': 'Not as spicy as other Yelpers led me to believe, so fear not; also excellent.', 'rating': 5}, {'name': 'Papardelle', 'description': 'Very good too.', 'rating': 5}, {'name': 'Lamb', 'description': 'Very good but a bit salty for my taste.', 'rating': 4}, {'name': 'Key Lime Pie', 'description': 'Not a huge dessert person, but places like Sbraga change me...', 'rating': 5}, {'name': 'Pistachio Bread Pudding with Olive Oil Ice cream', 'description': 'Super interesting flavor, and my roomate who hates normally ice cream loved it!', 'rating': 5}, {'name': 'Popovers', 'description': 'I would like for them to pop over to me anytime.', 'rating': 5}, {'name': 'Spicy cocktail', 'description': 'Very odd but very interesting.', 'rating': 4}, {'name': 'Peach Cobbler inspired cocktail', 'description': 'Also great, but a bit too sweet for me.', 'rating': 4}]}\n"]}],"source":["# Apply the function to each review\n","crf_df['word_tuples'] = crf_df.apply(lambda row: convert_text_to_tuples(row['text'], row['ner_results']), axis=1)\n","\n","# remove empty word tuples\n","crf_df = crf_df[crf_df['word_tuples'].apply(lambda x: len(x) != 0)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709581254731,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"5He20lrIqtSM","outputId":"41270bfa-cf17-4b30-8abc-9374fff86c09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total dataset size: 18748\n","Train set size: 14998\n","Test set size: 3750\n"]}],"source":["# split into train/test sets\n","crf_df_train, crf_df_test = train_test_split(crf_df, test_size=0.2, random_state=42, shuffle=True)\n","print(f\"Total dataset size: {len(crf_df)}\")\n","print(f\"Train set size: {len(crf_df_train)}\")\n","print(f\"Test set size: {len(crf_df_test)}\")"]},{"cell_type":"markdown","metadata":{"id":"VuvBD5snxBP4"},"source":["### Extract features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liONTyp8xG_z"},"outputs":[],"source":["def word2features(sent, i):\n","    word = sent[i][0]\n","    postag = sent[i][1]\n","\n","    features = {\n","        'bias': 1.0,\n","        'word.lower()': word.lower(),\n","        'word[-3:]': word[-3:],\n","        'word[-2:]': word[-2:],\n","        'word.isupper()': word.isupper(),\n","        'word.istitle()': word.istitle(),\n","        'word.isdigit()': word.isdigit(),\n","        'postag': postag,\n","        'postag[:2]': postag[:2],\n","    }\n","    if i > 0:\n","        word1 = sent[i-1][0]\n","        postag1 = sent[i-1][1]\n","        features.update({\n","            '-1:word.lower()': word1.lower(),\n","            '-1:word.istitle()': word1.istitle(),\n","            '-1:word.isupper()': word1.isupper(),\n","            '-1:postag': postag1,\n","            '-1:postag[:2]': postag1[:2],\n","        })\n","    else:\n","        features['BOS'] = True\n","\n","    if i < len(sent)-1:\n","        word1 = sent[i+1][0]\n","        postag1 = sent[i+1][1]\n","        features.update({\n","            '+1:word.lower()': word1.lower(),\n","            '+1:word.istitle()': word1.istitle(),\n","            '+1:word.isupper()': word1.isupper(),\n","            '+1:postag': postag1,\n","            '+1:postag[:2]': postag1[:2],\n","        })\n","    else:\n","        features['EOS'] = True\n","\n","    return features\n","\n","#################################################################\n","\n","import string\n","punctuation = set(string.punctuation)\n","\n","def is_punctuation(token):\n","    return token in punctuation\n","\n","def is_numeric(token):\n","    try:\n","        float(token.replace(\",\", \"\"))\n","        return True\n","    except:\n","        return False\n","\n","def word2features_1(sentence_frame, i):\n","    token = sentence_frame[i][0]\n","    postag = sentence_frame[i][1]\n","\n","    # current_token = sentence_frame.iloc[current_idx]\n","    # token = current_token['token']\n","    #position = current_token['position']\n","    #token_count = current_token['token_count']\n","    #pos = current_token['pos_tag']\n","\n","    # Shared features across tokens\n","    features = {\n","            'bias': True,\n","            'word.lower': token.lower(),\n","            'word.istitle': token.istitle(),\n","            'word.isdigit': is_numeric(token),\n","            'word.ispunct': is_punctuation(token)\n","           # 'word.position':position,\n","          #  'word.token_count': token_count,\n","          # 'postag': pos,\n","    }\n","\n","    if i > 0: # The word is not the first one...\n","        #prev_token = sentence_frame.iloc[current_idx-1]['token']\n","        # prev_pos = sentence_frame.iloc[current_idx-1]['pos_tag']\n","        prev_token = sentence_frame[i-1][0]\n","        prev_pos = sentence_frame[i-1][1]\n","        features.update({\n","            '-1:word.lower': prev_token.lower(),\n","            '-1:word.istitle':prev_token.istitle(),\n","            '-1:word.isdigit': is_numeric(prev_token),\n","            '-1:word.ispunct': is_punctuation(prev_token),\n","            '-1:postag':prev_pos\n","        })\n","    else:\n","        features['BOS'] = True\n","\n","    if i < len(sentence_frame) - 1: # The word is not the last one...\n","        #next_token = sentence_frame.iloc[current_idx+1]['token']\n","        #next_tag = sentence_frame.iloc[current_idx+1]['pos_tag']\n","        next_token = sentence_frame[i+1][0]\n","        next_tag = sentence_frame[i+1][1]\n","        features.update({\n","            '+1:word.lower': next_token.lower(),\n","            '+1:word.istitle': next_token.istitle(),\n","            '+1:word.isdigit': is_numeric(next_token),\n","            '+1:word.ispunct': is_punctuation(next_token),\n","            '+1:postag': next_tag\n","        })\n","    else:\n","        features['EOS'] = True\n","\n","    return features\n","\n","\n","def sent2features(sent):\n","    return [word2features(sent, i) for i in range(len(sent))]\n","\n","def sent2labels(sent):\n","    return [label for token, postag, label in sent]\n","\n","def sent2tokens(sent):\n","    return [token for token, postag, label in sent]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6737,"status":"ok","timestamp":1709581261465,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"zeDUPdOPxOBx","outputId":"bec6f38f-9eb5-4606-b2f0-481e61481e56"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 4.74 s, sys: 1.82 s, total: 6.56 s\n","Wall time: 6.69 s\n"]}],"source":["%%time\n","# Extract features from reviews\n","# X_train = crf_df_train.apply(lambda row: sent2features(row['word_tuples']), axis=1)\n","# y_train = crf_df_train.apply(lambda row: sent2labels(row['word_tuples']), axis=1)\n","X_train = [sent2features(s) for s in list(crf_df_train['word_tuples'])]\n","y_train = [sent2labels(s) for s in list(crf_df_train['word_tuples'])]\n","\n","X_test = [sent2features(s) for s in list(crf_df_test['word_tuples'])]\n","y_test = [sent2labels(s) for s in list(crf_df_test['word_tuples'])]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PVuDtqjFyY9C"},"source":["### Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1709581261465,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"iO0PjCopyWSa","outputId":"df2655c3-68f8-462a-dbc8-cb4f7b78fff8"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 210 µs, sys: 0 ns, total: 210 µs\n","Wall time: 280 µs\n"]}],"source":["%%time\n","trainer = pycrfsuite.Trainer(verbose=False)\n","\n","trainer.set_params({\n","    'c1': 1.0,   # coefficient for L1 penalty\n","    'c2': 1e-3,  # coefficient for L2 penalty\n","    'max_iterations': 200,\n","\n","    'feature.possible_transitions': True\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMNQr4meUl4N"},"outputs":[],"source":["# We are feeding our training set to the algorithm here.\n","for xseq, yseq in zip(X_train, y_train):\n","    trainer.append(xseq, yseq)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":147065,"status":"ok","timestamp":1709581429173,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"y0siBxPRf1NP","outputId":"332da60e-ff90-4329-f006-0c9242d83938"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 2min 25s, sys: 819 ms, total: 2min 26s\n","Wall time: 2min 27s\n"]}],"source":["%%time\n","trainer.train(dataset_root + '../Model/crf_train_model_ner.crfsuite')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169,"status":"ok","timestamp":1709581429338,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"mGwxzRCWxY_2","outputId":"79f1ccc1-3bb2-4282-8ed1-3fb210308c58"},"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/drive/MyDrive/UMich Milestone II Project/Final_Code_Submission/Dataset/NER/Yelp/../Model/crf_train_model_ner.crfsuite'\n"]}],"source":["!ls '/content/drive/MyDrive/UMich Milestone II Project/Final_Code_Submission/Dataset/NER/Yelp/../Model/crf_train_model_ner.crfsuite'"]},{"cell_type":"markdown","metadata":{"id":"R-apyJXM6e0m"},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1709581429338,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"iKtDTsNhgQHd","outputId":"14995815-95ac-4e37-ef51-faeaea4ee1f9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<contextlib.closing at 0x78626f5b8d60>"]},"metadata":{},"execution_count":16}],"source":["crf_tagger = pycrfsuite.Tagger()\n","crf_tagger.open(dataset_root + '../Model/crf_train_model_ner.crfsuite')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ooVE6QaRiooJ","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1709581429338,"user_tz":480,"elapsed":6,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"}},"outputId":"bbad1bc5-6ced-4692-aa94-885ba5a6018b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfor i in range(len(X_test)):\\n  predicted_tags = crf_tagger.tag(X_test[i])\\n  if predicted_tags == y_test[i]:\\n    print(i)\\n    '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["'''\n","for i in range(len(X_test)):\n","  predicted_tags = crf_tagger.tag(X_test[i])\n","  if predicted_tags == y_test[i]:\n","    print(i)\n","    '''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709581429339,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"},"user_tz":480},"id":"7vGgIdX94VSV","outputId":"0817b676-4c0d-4446-b4cd-72ce4f69e7ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Review text: - TIP:  Their black garlic wings are some of the best wings in the city.\n","\n","- CON:  Recent happy hour addition was downright disappointing.  Quality of dishes wasn't up to Cheu's standard.  Service was curt.  \n","\n","- BOTTOM LINE:  It pains me to give this place 3 stars.  A year ago, I emphetically would proclaim it to be my favorite restaurant in the city.  But they've lost their way.  Hand torn noodles are gone from the menu.  Service has been shoddy (it used to be extremely welcoming).  Food quality has been inconsistent.  It's a shame, because I want to keep on loving this place. \n","\n","Extected NER label: {'food': ['black garlic wings'], 'drink': []}\n","found food item: black\n","found food item: garlic\n","found food item: wings\n","found food item: wings\n"]}],"source":["# Examine review 574\n","REV_IDX = 574\n","print(f\"Review text: {crf_df_test.iloc[REV_IDX].text} \\n\")\n","print(f\"Extected NER label: {crf_df_test.iloc[REV_IDX].ner_results}\")\n","\n","response = crf_tagger.tag(X_test[REV_IDX])\n","#crf_df_test.iloc[REV_IDX].text.split()[res.index('food')]\n","#X_test[REV_IDX][res.index('food')]\n","\n","\n","for i, tag in enumerate(response):\n","    if tag == 'food':\n","        print(f\"found food item: {X_test[REV_IDX][i]['word.lower()']}\")\n","    if tag == 'drink':\n","        print(f\"found drink item: {X_test[REV_IDX][i]['word.lower()']}\")\n"]},{"cell_type":"markdown","metadata":{"id":"aIi34jYsjc5m"},"source":["### Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8QzMgIegsz8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709581435720,"user_tz":480,"elapsed":6385,"user":{"displayName":"Sangram Sanjiv Malekar","userId":"14731348828998554218"}},"outputId":"cd673ce8-8f34-4f95-9b3b-91f8106bcb30"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        None       0.96      0.98      0.97    203007\n","       drink       0.57      0.31      0.40      2404\n","        food       0.65      0.47      0.54     13746\n","\n","    accuracy                           0.94    219157\n","   macro avg       0.73      0.59      0.64    219157\n","weighted avg       0.93      0.94      0.94    219157\n","\n"]}],"source":["all_true, all_pred = [], []\n","\n","for i in range(len(X_test)):\n","    all_true.extend(y_test[i])\n","    all_pred.extend(crf_tagger.tag(X_test[i]))\n","\n","print(classification_report(all_true, all_pred))"]}],"metadata":{"colab":{"collapsed_sections":["PVuDtqjFyY9C"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}